---
layout: minimal
title: Bayesian Image Reconstruction using Deep Generative Models
published: true
---


<center><h1>Bayesian Image Reconstruction using Deep Generative Models</h1></center>
<center><h4>Razvan Marinescu, Daniel Moyer, Polina Golland</h4></center>
<center><h4>MIT CSAIL</h4></center>


![diagram](https://i.imgur.com/Nb0123s.png)

* Pre-print: [arxiv](https://arxiv.org/abs/2012.04567)
* Code: 
  * [Tensorflow](https://github.com/razvanmarinescu/brgm)
  * [PyTorch](https://github.com/razvanmarinescu/brgm-pytorch) (+ variational inference)
* Demos through Colab notebooks: 
  * [Tensorflow](https://colab.research.google.com/drive/1G7_CGPHZVGFWIkHOAke4HFg06-tNHIZ4?usp=sharing)
  * [Pytorch Bayesian MAP](https://colab.research.google.com/drive/1xJAor6_Ky36gxIk6ICNP--NMBjSlYKil?usp=sharing)
  * [Pytorch Variational Inference](https://colab.research.google.com/drive/1dDH_bV3aYPSK1ujbPg9MBYf-7_Lbhyrw?usp=sharing)
  * <span style="color:red">NEW [Try BRGM on your own image](https://colab.research.google.com/drive/1dDH_bV3aYPSK1ujbPg9MBYf-7_Lbhyrw?usp=sharing)


## Abstract
 
```
Machine learning models are commonly trained end-to-end and in a supervised setting, using 
paired (input, output) data. Examples include recent super-resolution methods that train on 
pairs of (low-resolution, high-resolution) images. However, these end-to-end approaches 
require re-training every time there is a distribution shift in the inputs (e.g., night 
images vs daylight) or relevant latent variables (e.g., camera blur or hand motion). In 
this work, we leverage state-of-the-art (SOTA) generative models (here StyleGAN2) for 
building powerful image priors, which enable application of Bayes' theorem for many 
downstream reconstruction tasks. Our method, Bayesian Reconstruction through Generative 
Models (BRGM), uses a single pre-trained generator model to solve different image restoration 
tasks, i.e., super-resolution and in-painting, by combining it with different forward 
corruption models. We keep the weights of the generator model fixed, and reconstruct the 
image by estimating the Bayesian maximum a-posteriori (MAP) estimate over the input latent 
vector that generated the reconstructed image. We further use variational inference to 
approximate the posterior distribution over the latent vectors, from which we sample multiple 
solutions. We demonstrate BRGM on three large and diverse datasets: (i) 60,000 images from 
the Flick Faces High Quality dataset (ii) 240,000 chest X-rays from MIMIC III and (iii) a 
combined collection of 5 brain MRI datasets with 7,329 scans. Across all three datasets and 
without any dataset-specific hyperparameter tuning, our simple approach yields performance 
competitive with current task-specific state-of-the-art methods on super-resolution and 
in-painting, while being more generalisable and without requiring any training. Our source 
code and pre-trained models are available online: 
```  
  
## News

* **Next**: 
	* 3D reconstruction for medical scans
	* More reconstruction tasks: e.g. MRI compressed sensing
* **June 2021**: Reimplemented the method in Pytorch, and switched to StyleGAN-ADA.
* **May 2021**: Added variational inference extension for sampling multiple solutions. Updated methods section in [arXiv paper](https://arxiv.org/abs/2012.04567). Also included qualitative comparisons against Deep Image Prior in supplement.
* **Feb 2021**: Updated methods section in [arXiv paper](https://arxiv.org/abs/2012.04567). We now start from the full Bayesian formulation, and derive the loss function from the MAP estimate (in appendix), and show the graphical model. Code didn't change in this update.
* **Dec 2020**: Code and pre-trained models published on github
* **Nov 2020**: Uploaded article pre-print to [arXiv](https://arxiv.org/abs/2012.04567).


If you use our model, please cite:
```
@article{marinescu2020bayesian,
  title={Bayesian Image Reconstruction using Deep Generative Models},
  author={Marinescu, Razvan V and Moyer, Daniel and Golland, Polina},
  journal={arXiv preprint arXiv:2012.04567},
  year={2020}
}
```


## Main results, as twitter thread

<a class="twitter-timeline" data-width="600" data-height="800" href="https://twitter.com/RazMarinescu/timelines/1346199054053609472?ref_src=twsrc%5Etfw">BRGM - Curated tweets by RazMarinescu</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

### Mistake fixed in the pre-print (full thread on Twitter)

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">In science, it&#39;s very important to admit mistakes and correct them. I recently made a claim in my Bayesian image reconstruction paper that turned out to be wrong /1<a href="https://twitter.com/hashtag/ScienceMistakes?src=hash&amp;ref_src=twsrc%5Etfw">#ScienceMistakes</a></p>&mdash; Razvan Marinescu (@RazMarinescu) <a href="https://twitter.com/RazMarinescu/status/1364270228071321601?ref_src=twsrc%5Etfw">February 23, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



## Funding

This project received funding from the NIH grants NIBIB NAC P41EB015902 and NINDS R01NS086905.

 
